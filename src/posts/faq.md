---
title: FAQ
description: Questions fréquemment posées sur Pause IA et les risques de l'IA superintelligente.
original:
  title: FAQ
  url: https://github.com/PauseAI/pauseai-website/blob/5950ca0185d6e7cf52e2416e795acfab7222dace/src/posts/faq.md
---

<!-- ↓↓↓ NE PAS TOUCHER ↓↓↓ -->
<script lang="ts">
  import Accordion from '$components/Accordion.svelte'
	import { page } from '$app/stores'

  $: toc = $page.url.pathname === '/faq'

</script>

{#if toc}

<!-- ↑↑↑ NE PAS TOUCHER ↑↑↑ -->

## Sommaire

- [Qui êtes-vous ?](#accordion1)
- [N’êtes-vous pas tout simplement des technophobes ?](#accordion2)
- [Voulez-vous interdire toute forme d'IA ?](#accordion3)
- [Croyez-vous que GPT-4 va tous nous tuer ?](#accordion4)
- [Un moratoire ne risque-t-il pas d'aggraver les choses ?](#accordion5)
- [Un moratoire est-il possible ?](#accordion6)
- [Qui vous finance ?](#accordion7)
- [Quels sont vos projets ?](#accordion8)
- [Comment comptez-vous convaincre les gouvernements d'arrêter temporairement le développement de l'IA ?](#accordion9)
- [Pourquoi manifester ?](#accordion10)
- [Quelle est la probabilité que l'apparition d'une superintelligence ait de graves conséquences, y compris un risque d'extinction ?](#accordion11)
- [Combien de temps nous reste-t-il avant l'émergence d'une superintelligence ?](#accordion12)
- [Si nous appliquons un moratoire, qu'en est-il de la Chine ?](#accordion13)
- [OpenAI et Google semblent appeler de leurs voeux une réglementation. Pourquoi s'opposer à eux ?](#accordion14)
- [Les entreprises d'IA prétendent-elles que le risque existentiel est réel pour nous manipuler ?](#accordion15)
- [Je veux vous aider ! Que puis-je faire ?](#accordion16)

<!-- ↓↓↓ NE PAS TOUCHER ↓↓↓ -->

{/if}

<!-- ↑↑↑ NE PAS TOUCHER ↑↑↑ -->

### Qui êtes-vous ?

Nous sommes un ensemble de bénévoles ressemblés au sein d'une [association à but non lucratif](/mentions-legales) dont l'objectif est de minimiser les [risques liés à l'IA](/dangers) (y compris le [risque d'extinction](/dangers/pour-l'humanite)). Notre objectif est de convaincre le gouvernement d'intervenir et [de mettre en pause le développement d'une IAG](/propositions) (Intelligence Artificielle Générale puis d’une Superintelligence). Dans ce but, nous alertons le public, dialoguons avec les décideurs et organisons des manifestations.

Pause IA est la branche française de PauseAI Global. Bien que nous soyons en relation avec PauseAI Global, nous bénéficions d'une entière autonomie et menons nos propres projets en France. Apprenez-en plus sur nos [membres](/qui-sommes-nous).
Vous pouvez consulter notre [charte des valeurs](/charte-des-valeurs) pour en savoir plus sur nos principes et engagements.

Vous pouvez nous rejoindre sur [Discord](https://discord.gg/vyXGd7AeGc) (le coeur de nos discussions et actions), [Twitter](https://twitter.com/pause_ia), [Facebook](https://www.facebook.com/Pause.IA), [TikTok](https://www.tiktok.com/@pause_ia), [LinkedIn](https://www.linkedin.com/company/pause-ia/), [YouTube](https://www.youtube.com/@Pause_IA), [Instagram](https://www.instagram.com/pause_ia) et [Threads](https://www.threads.net/@pause_ia). Vous pouvez également nous contacter par mail à [contact@pauseia.fr](mailto:contact@pauseia.fr).

### N’êtes-vous pas tout simplement des technophobes ?

Vous seriez surpris d'apprendre que la majorité des membres de Pause IA sont en réalité des passionnés de progrès technologique. Beaucoup d'entre eux sont impliqués dans le développement de l'IA, sont des enthousiastes des nouvelles technologies et ont longtemps envisagé l'avenir avec optimisme. Ils s'intéressaient particulièrement au potentiel extraordinaire de l'IA pour le développement de l'humanité.

C'est précisément pour cette raison que la prise de conscience des risques existentiels liés à l'IA a été si difficile à accepter pour nombre d'entre eux. Leur engagement actuel ne découle pas d'une peur irrationnelle de la technologie, mais d'une compréhension approfondie des enjeux et d'un désir sincère de garantir que le développement de l'IA serve véritablement l'humanité.

### Voulez-vous interdire toute forme d'IA ?

Non. Nous voulons uniquement interdire le développement des plus gros systèmes d'IA à usage général souvent appelés « modèles de pointe ». La quasi-totalité des modèles existants, ainsi que la plupart des futurs modèles d'IA, resteraient [légaux selon notre proposition](/propositions). Nous demandons l'interdiction des systèmes plus puissants que GPT-4o, jusqu'à ce que nous puissions exercer un contrôle démocratique sur ces modèles et que nous soyons en mesure de les créer en toute sécurité.

### Croyez-vous que GPT-4 va tous nous tuer ?

Non, nous ne croyons pas que les modèles actuels représentent un risque existentiel et probablement que la plupart des prochains modèles non plus. Mais si nous poursuivons le développement de systèmes toujours plus puissants, nous atteindrons un point de non-retour où l'un d'eux deviendra [une menace existentielle](/dangers/pour-l'humanite).

### Un moratoire ne risque-t-il pas d'aggraver les choses ?

PauseAI Global a répondu à ces préoccupations [dans cet article](https://pauseai.info/mitigating-pause-failures).

### Un moratoire est-il possible ?

L’émergence d’une superintelligence n'est pas inévitable. Sa création nécessite des armées d'ingénieurs payés à coup de millions de dollars et une chaîne d'approvisionnement de matériel de pointe non réglementé. Sa création implique aussi que nous permettions à ces entreprises de jouer avec notre avenir en restant passifs.

PauseAI Global : [en savoir plus sur la faisabilité d’un moratoire](https://pauseai.info/feasibility)

### Qui vous finance ?

Depuis juin 2024, Pause IA est une [organisation à but non lucratif enregistrée](/mentions-legales) et nous sommes activement à la recherche de financements. Vous pouvez [faire un don à Pause IA](/dons) si vous souhaitez soutenir notre cause. Nous utilisons l'essentiel de l'argent pour l'organisation d'événements impactants, des campagnes de sensibilisation et la formalisation de notre structure.

### Quels sont vos projets ?

Nous nous concentrons sur la croissance du mouvement, la production de contenu éducatif, la mise en relation avec des journalistes et influenceurs, l'organisation de manifestations et le lobbying auprès des politiques.

### Comment comptez-vous convaincre les gouvernements d’arrêter temporairement le développement de l'IA ?

Jetez un œil à la "[Théorie du changement](https://pauseai.info/theory-of-change)" de PauseAI Global pour un aperçu détaillé de notre stratégie.

### Pourquoi manifester ?

- Manifester démontre à tous que cette question nous tient à cœur. En manifestant, nous prouvons que nous sommes prêts à consacrer du temps et de l'énergie à la diffusion de notre message.
- Il n’est pas rare que les manifestations aient une [influence positive](https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf) sur l’opinion publique, le vote, l’attitude des entreprises et la loi.
- [La grande majorité des gens soutiennent](https://today.yougov.com/politics/articles/31718-do-protesters-want-help-or-hurt-america) les manifestations pacifiques et non violentes.
- Il n'y a [aucun "retour de bâton"](https://journals.sagepub.com/doi/full/10.1177/2378023120925949) sauf si la manifestation [dégénère en violences](https://news.stanford.edu/stories/2018/10/how-violent-protest-can-backfire). Nos manifestations sont pacifiques et non violentes.
- C'est une expérience de lien social. Vous rencontrez d'autres personnes qui partagent vos préoccupations et votre volonté d'agir.
- Lisez [cet excellent article](https://forum.effectivealtruism.org/posts/4ez3nvEmozwPwARr9/a-case-for-the-effectiveness-of-protest) pour en savoir plus sur l'efficacité des manifestations.

### Quelle est la probabilité que l'apparition d'une superintelligence ait de graves conséquences, y compris un risque d'extinction ?

PauseAI Global a compilé [une liste de valeurs "p(doom)"](https://pauseai.info/pdoom) (probabilité de scénarios catastrophiques) provenant de divers experts renommés dans le domaine.

Les chercheurs en sécurité de l'IA (qui sont experts du sujet) sont partagés, [leurs estimations allant de 2% à 97% avec une moyenne de 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results). Notez qu'aucun des chercheurs en sécurité interrogés ne croit en une probabilité de 0%. Un biais de sélection est cependant possible : ceux qui travaillent dans le domaine de la sécurité de l'IA le font probablement car ils redoutent les conséquences néfastes de l'IA.

Si l'on interroge les chercheurs en IA en général (qui ne sont pas spécialistes en sécurité), ce chiffre tombe à [une moyenne d'environ 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/), avec une médiane de 5%. Une minorité, environ 20%, pense que le problème d'alignement n'est ni réel ni important. Là encore, un biais de sélection inverse est possible : ceux qui travaillent en IA le font sans doute car ils considèrent que les conséquences de l'IA sur le monde seront uniquement bénéfiques.

_Imaginez qu'on vous propose d'essayer un nouvel avion._ Les ingénieurs estiment les risques de crash à 14%. Monteriez-vous à bord ? C'est plus ou moins la situation actuelle alors que nous embarquons tous à bord du même avion.

### Combien de temps nous reste-t-il avant l'émergence d'une superintelligence ?

Cela pourrait prendre des mois, ou bien des décennies, personne n'en est certain. Ce que nous savons, c'est que les progrès dans le domaine de l'IA sont souvent largement sous-estimés. Il y a seulement trois ans, nous pensions qu'il faudrait attendre 2055 pour voir des modèles capables de réussir un test SAT (équivalent PISA aux Etats-Unis). Nous y sommes parvenus dès avril 2023. Il semble souhaitable d’agir comme si le temps nous était compté afin de ne pas être pris au dépourvu.

[En savoir plus sur l'urgence de la situation.](https://pauseai.info/urgency)

### Si nous appliquons un moratoire, qu'en est-il de la Chine ?

La Chine a actuellement les réglementations les plus strictes au monde en matière d'IA. Les [chatbots sont interdits](https://www.reuters.com/technology/chinas-slow-ai-roll-out-points-its-tech-sectors-new-regulatory-reality-2023-07-12/) et [l'entraînement sur des données internet n'était pas autorisé](https://cointelegraph.com/news/china-sets-stricter-rules-training-generative-ai-models) jusqu'en [septembre 2023](https://asia.nikkei.com/Business/Technology/China-approves-AI-chatbot-releases-but-will-it-unleash-innovation). Le gouvernement chinois, avec son mode de régime autoritaire, a bien plus de raisons de craindre les impacts incontrôlables et imprévisibles de l'IA que nous. Lors de la réunion du Conseil de sécurité des Nations unies sur la sécurité de l'IA, la Chine a été le seul pays à mentionner la possibilité d’instaurer un moratoire.

De plus, nous appelons à un moratoire international, imposé par un traité. Un tel traité doit également être signé par la Chine. Si le traité garantit que d'autres nations s'arrêteront aussi, et qu'il y a des mécanismes de contrôle et des mesures de mise en vigueur suffisantes, la Chine y sera probablement favorable.

### OpenAI et Google semblent appeler de leurs vœux une réglementation. Pourquoi s’opposer à eux ?

Nous saluons les appels [d'OpenAI](https://openai.com/index/governance-of-superintelligence/) et de [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) pour demander une réglementation internationale vis-à-vis de l'IA. Cependant, nous pensons que les propositions actuelles ne suffiront pas à éviter une catastrophe. Google et Microsoft n'ont pas encore reconnu publiquement les risques existentiels liés à l'IA. Seul OpenAI [mentionne explicitement le risque d'extinction](https://openai.com/index/governance-of-superintelligence/). Cependant, leur stratégie est très claire: un moratoire est impossible, nous devons d'abord créer une superintelligence avant de penser à de sérieuses régulations. Mais ils avouent eux-mêmes [ne pas avoir résolu le problème d'alignement](https://www.youtube.com/watch?t=1478&v=L_Guz73e6fw&feature=youtu.be) et les derniers développements prouvent [qu'ils ne traitent pas ce problème avec le sérieux qu'il mérite](https://www.cnbc.com/2024/05/17/openai-superalignment-sutskever-leike.html). Ces entreprises sont engagées dans une course contre-la-montre au détriment de la sécurité, sacrifiée pour un avantage concurrentiel. C'est le résultat de la dynamique du marché. Nous devons encourager les gouvernements à intervenir et à mettre en place des politiques internationales pour [éviter les pires scénarios](/propositions).

### Les entreprises d'IA prétendent-elles que le risque existentiel est réel pour nous manipuler ?

Nous ne pouvons pas être certains des motivations de ces entreprises, et nous savons qu'elles **ne sont pas à l'origine de la mise en avant des risques existentiels liés à l'IA**. Les signaux d'alerte venaient des scientifiques, militants et ONG. Jetons un œil à la chronologie.

Depuis le début des années 2000, diverses personnes ont alerté sur ce risque existentiel, comme Eliezer Yudkowsky, Nick Bostrom, Stuart Russell, Max Tegmark, et bien d'autres. Ils n'avaient aucun produit à vendre, ils étaient simplement préoccupés par l'avenir de l'humanité.

Les entreprises d'IA n'ont commencé à mentionner les risques existentiels que très récemment.

Sam Altman est une exception. Sur son blog personnel, il a exploré [l'idée du risque existentiel dès 2015](https://blog.samaltman.com/machine-intelligence-part-1), avant même de fonder OpenAI. Pendant les années qui ont suivi, il n'a quasiment plus jamais évoqué explicitement ce risque. Lors de son audition devant le Sénat Américain le 16 mai 2023, interrogé sur cet article de blog, il a soigneusement évité la question en préférant parler des emplois et de l'économie.

En mai 2023, tout a changé :

- Le 1er mai, le pionnier de l'IA Geoffrey Hinton [démissionne de Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) pour alerter le public sur la possibilité de risques existentiels.
- Le 20 mai, [la première manifestation de PauseAI Global](https://pauseai.info/openai-protest) a lieu devant le siège d'OpenAI.
- Le 22 mai, OpenAI publie [un article de blog sur la gouvernance de la superintelligence](https://openai.com/index/governance-of-superintelligence/), et mentionne le risque existentiel pour la première fois.
- Le 24 mai, l'ancien PDG de Google, Eric Schmidt, reconnaît la possibilité de risques existentiels.
- Le 30 mai, le centre pour la sécurité de l'IA publie [une déclaration sur les risques existentiels](https://www.safe.ai/work/statement-on-ai-risk), incluant des employés d'OpenAI, Google et Microsoft.

Ces entreprises ont été lentes à reconnaître la possibilité de risques existentiels, alors que beaucoup de leurs employés en étaient conscients depuis des années. Selon nous, les entreprises d'IA ont simplement réagi à l'émergence des risques existentiels dans le discours public et n'ont apporté leur réponse que lorsque le sujet est devenu inévitable.

Mais les incitations commerciales vont à contre-courant : il n'est pas dans l'intérêt de ces entreprises que le public s'inquiète des dangers de leurs produits. Presque toutes minimisent les risques pour attirer clients et investisseurs. Combien de régulations et de négativité risquent-elles d'attirer en admettant ces dangers ? Et une entreprise comme OpenAI consacrerait-elle [20% de sa puissance de calcul informatique](https://openai.com/index/introducing-superalignment/) à la sécurité de l'IA si elle ne croyait pas en ces dangers ?

Notre interprétation est que les entreprises d'IA ont signé cette déclaration parce qu'_elles savent que les risques existentiels sont un problème à prendre très au sérieux_.

Une raison majeure pour laquelle de nombreuses personnes ne veulent toujours pas croire que les risques existentiels sont une préoccupation réelle est que la reconnaissance d'un tel danger est une énorme charge mentale.

[PauseAI Global : en savoir plus sur la charge mentale des risques existentiels](https://pauseai.info/psychology-of-x-risk)

### Je veux aider ! Que puis-je faire ?

Il y a de nombreuses choses que [vous pouvez faire](/agir). À titre individuel, sensibilisez votre entourage, [faites un don](/dons) et [rejoignez Pause IA](/nous-rejoindre) pour coordonner vos actions avec tous nos membres. Si vous souhaitez vous impliquer davantage, vous pouvez aussi devenir bénévole.

Même confrontés à la perspective de risques existentiels, il reste de l'espoir mais surtout du travail à accomplir.
