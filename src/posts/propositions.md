---
title: Propositions de Pause IA
description: Mettre en place un moratoire sur l'entraînement des systèmes d'IA plus puissants que GPT-4, interdire l'entraînement des systèmes d'IA sur du matériel sous droit d'auteur, tenir les créateurs de modèles d'IA responsables.
original:
  title: PauseAI Proposal
  url: https://github.com/PauseAI/pauseai-website/blob/0cabfef1037097bd7a99f4fddd1bcd73abbe6760/src/posts/proposal.md
---

**Instaurer un moratoire sur l'entraînement des systèmes d'IA plus puissants que GPT-4** jusqu'à ce que nous sachions comment les développer en toute sécurité et les maintenir sous contrôle démocratique.

Chaque pays peut et doit mettre en œuvre cette mesure _immédiatement_. En particulier, les États-Unis (ou la Californie spécifiquement) devraient instaurer un moratoire, puisqu'ils hébergent pratiquement toutes les grandes entreprises d'IA. De nombreux scientifiques et leaders de l'industrie [conviennent qu'un moratoire est nécessaire](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), et le grand public soutient également une telle mesure ([64%](https://www.campaignforaisafety.org/usa-ai-x-risk-perception-tracker/) - [69%](https://today.yougov.com/topics/technology/survey-results/daily/2023/04/03/ad825/2)).

Cependant, nous ne devons pas nous attendre à ce que des pays ou des entreprises prennent le risque de perdre leur avantage concurrentiel en mettant en pause l’entraînement de leurs modèles d’IA si d'autres ne font pas de même. C'est pourquoi nous avons besoin d'un **moratoire** **international**.

## Mettre en œuvre un moratoire international

Un accord international se conclut généralement lors d'un sommet, où les dirigeants des différents pays se réunissent pour discuter du problème et prendre une décision. Le Royaume-Uni a pris les devants et a organisé un sommet sur la sécurité de l'IA à l'automne 2023. Depuis, deux autres sommets ont été annoncés ([plus d'informations sur les sommets](https://pauseai.info/summit)).

L'objectif principal du sommet devrait être un **traité**. Ce traité devra spécifier les mesures politiques qui nous protègent des [risques liés à l'IA](/dangers). Il doit être signé par tous les États membres de l'ONU.

### Agence internationale de sécurité de l'IA

Similaire à l'AIEA (Agence Internationale de l'Énergie Atomique), les prérogatives de cette agence seraient :

- Accorder les autorisations pour le _déploiement_ d'IA. Cela inclura des évaluations/tests approfondis des modèles.
- Accorder les autorisations pour de _nouvelles sessions d'entraînement_ de modèles d'IA dépassant une certaine taille (par exemple : 1 milliard de paramètres).
- Organiser des rencontres périodiques pour discuter des progrès de la recherche sur la sécurité de l'IA.

### Entraînement de systèmes d'IA générale

Autoriser l'entraînement de systèmes d'IA générale plus puissants que GPT-4 uniquement si leur sécurité peut être garantie :

- Par « plus puissants que GPT-4 », nous entendons tous les modèles d'IA qui sont soit 1) de taille supérieure à 10^12 paramètres, 2) utilisent plus de 10^25 FLOP pour l'entraînement, ou 3) dont les capacités s’apprêtent à dépasser celles de GPT-4.
- Ces prérogatives ne ciblent pas les systèmes d'IA _étroits_ (dits spécialisés), comme la reconnaissance d'images utilisée pour diagnostiquer des problèmes médicaux.
- Exiger une [supervision pendant les sessions d'entraînement.](https://www.alignmentforum.org/posts/Zfk6faYvcf5Ht7xDx/compute-thresholds-proposed-rules-to-mitigate-risk-of-a-lab)
- La sécurité ne peut être garantie que par l’existence d’un fort consensus scientifique et par [la preuve](https://arxiv.org/abs/2309.01933) que le _problème de l’alignement a été résolu_. Pour l'instant, ce n'est pas le cas, nous ne devrions donc pas permettre l'entraînement de tels systèmes.
- Il se peut que le problème de l’alignement de l'IA ne soit jamais résolu - il pourrait être insoluble. Dans ce cas, nous ne devrions jamais autoriser l'entraînement de tels systèmes.
- Même si nous pouvons construire une IA sûre et contrôlable, ne développer et déployer cette technologie qu'avec un **contrôle démocratique** solide. Une superintelligence est trop puissante pour être contrôlée par une seule entreprise ou un seul pays.
- **[Surveiller les ventes de GPU](https://arxiv.org/abs/2303.11341)** et autre matériel pouvant être utilisés pour entraîner des IA.

### Déploiement des modèles

Autoriser le déploiement de modèles seulement après avoir obtenu l’assurance qu'ils ne présentent aucune [capacité dangereuse](https://pauseai.info/dangerous-capabilities) :

- Nous aurons besoin de normes et d'équipes d'évaluation indépendantes pour déterminer si un modèle possède des capacités dangereuses.
- La liste des capacités à risque pourra évoluer avec la croissance des capacités de l'IA.
- Noter que se fier uniquement aux évaluations de modèles [n'est pas suffisant](https://pauseai.info/4-levels-of-ai-regulation).

Mettre en place un moratoire peut avoir des effets délétères s'il n'est pas implémenté correctement ([en savoir plus sur la façon d'atténuer ces risques](https://pauseai.info/mitigating-pause-failures)).

## **Autres mesures pour un ralentissement effectif**

- **Interdire l'entraînement des systèmes d'IA sur du matériel sous droit d'auteur.** Cela résout les problèmes de droits d'auteur, ralentit l'accroissement des inégalités et freine la progression vers la superintelligence.
- **Tenir les créateurs de modèles d'IA responsables** des actes criminels commis à l'aide de leurs systèmes. Cela les incitera à s'assurer que leurs modèles sont sûrs.

## **Politique à long terme**

A l’heure où ce texte est rédigé, l'entraînement d'un modèle de la taille de GPT-3 coûte plusieurs millions de dollars. Cela rend l'entraînement de tels modèles très difficile, et facilite son contrôle via le traçage des GPUs. Cependant, le coût d'entraînement diminue de façon exponentielle avec l'amélioration des composants matériels et l'arrivée de nouveaux algorithmes d'entraînement.

Arrivera un moment où des modèles d'IA potentiellement superintelligents pourront être entraînés pour quelques milliers de dollars ou moins, peut-être même sur du matériel grand public. Nous devons nous y préparer. Nous devrions envisager les politiques suivantes :

- **Limiter la publication des algorithmes d'entraînement / d'améliorations de performance.** Parfois, la publication d'un nouvel algorithme rend l'entraînement beaucoup plus efficace. L'architecture « _Transformers_ », par exemple, a permis pratiquement tous les progrès récents en IA. Ces bonds en capacité peuvent survenir à tout moment, et nous devrions envisager de limiter la publication de tels algorithmes pour minimiser le risque d'un bond soudain en capacité. De même, certaines innovations en termes de performance pourraient changer radicalement ce qui est réalisable au moyen des modèles existants. Ces avancées pourraient elles aussi nécessiter d'être réglementées.
- **Limiter l’augmentation des capacités de calcul.** Si l'entraînement d'une superintelligence devient possible sur du matériel grand public, nous serons dans une situation délicate. Nous devrions envisager de limiter les progrès des capacités de calcul des processeurs graphiques destinés au grand public.

## **Aidez-nous à réaliser cet objectif**

[Rejoignez](/nous-rejoindre) le mouvement pour nous aider ou [agissez](/agir) par vous-même !
